{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7391bece-6eed-483d-80b9-6c5ca74a053c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "from transformers import pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf82f405-f8fe-4e80-b892-5d9390067ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 2 pages\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['1. What is Gradient descent? Gradient descent in a neural network is a fundamental optimization technique used to train the network by iteratively adjusting its parameters (weights and biases) to minimize a loss function, which quantifies the error between the network\\'s predictions and the actual target values; the process begins with a forward pass, where input data is propagated through the network layer by layer, each layer applying weighted sums followed by non-linear activation functions (like ReLU or sigmoid) to produce outputs, culminating in a final prediction. The loss function (such as mean squared error for regression or cross-entropy for classification) is then computed to measure how far off the predictions are from the ground truth; next comes the backward pass, known as backpropagation, where the network calculates the gradient of the loss with respect to each parameter by applying the chain rule of calculus through each layer in reverse â€”from output back to inputâ€”so that it can determine how much each weight contributed to the error; these gradients indicate the direction in which each weight should be changed to reduce the loss, and in the update step, each parameter is adjusted slightly in the opposite direction of its gradient (hence \"descent\") using a learning rate, which controls the size of these updates to ensure convergence without overshooting; this cycle of forward pass, loss computation, backpropagation, and weight update is repeated across many epochs over the training dataset, allowing the neural network to iteratively learn patterns in the data, gradually reducing the loss and improving its ability to generalize to unseen inputs, provided the model architecture is appropriate and overfitting or underfitting is mitigated through techniques like regularization, dropout, or validation.',\n",
       " '2. What is Regulariztion? Explain briefly the types of regularization Regularization in neural networks is a set of techniques used to prevent overfitting, which occurs when the model learns not only the underlying patterns in the training data but also the noise or random fluctuations, leading to poor generalization on unseen data; the core idea behind regularization is to add constraints or penalties to the model\\'s complexity during training, typically by modifying the loss function so that, in addition to minimizing prediction error, the model is also discouraged from having excessively large weights or overly complex behavior; one common form is L2 regularization (also called weight decay), where a term proportional to the sum of the squares of all weights is added to the loss function, effectively penalizing large weights and encouraging the network to distribute learning more evenly across all features, resulting in smoother and more general decision boundaries. Another variant is L1 regularization, which adds the absolute values of weights to the loss, promoting sparsity by pushing some weights toward zero and effectively performing feature selection; beyond these, dropout is a powerful regularization technique where, during training, a random subset of neurons is \"dropped\" (i.e., temporarily ignored) in each forward and backward pass, which forces the network to not rely too heavily on specific paths and promotes redundancy and robustness in learning; early stopping is another method where training is halted once performance on a validation set stops improving, thus avoiding the point at which the model starts to overfit; collectively, these regularization strategies are crucial for building models that generalize well, especially in deep learning where the number of parameters is large and the risk of overfitting is high if the model becomes too specialized on the training set.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_path = \"test.pdf\" \n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'(?<!\\n)\\n(?!\\n)', ' ', text)  \n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "    \n",
    "text_chunks = []\n",
    "with pdfplumber.open(pdf_path) as pdf:\n",
    "    \n",
    "    for i, page in enumerate(pdf.pages):\n",
    "        text = page.extract_text()\n",
    "        if text:\n",
    "            text_chunks.append(clean_text(text))\n",
    "\n",
    "print(f\"Extracted {len(text_chunks)} pages\")\n",
    "text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f2c1e6-4b14-4560-abfb-3f17a2dd1b9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. What is Gradient descent?',\n",
       " \"Gradient descent in a neural network is a fundamental optimization technique used to train the network by iteratively adjusting its parameters (weights and biases) to minimize a loss function, which quantifies the error between the network's predictions and the actual target values; the process begins with a forward pass, where input data is propagated through the network layer by layer, each layer applying weighted sums followed by non-linear activation functions (like ReLU or sigmoid) to produce outputs, culminating in a final prediction.\",\n",
       " 'The loss function (such as mean squared error for regression or cross-entropy for classification) is then computed to measure how far off the predictions are from the ground truth; next comes the backward pass, known as backpropagation, where the network calculates the gradient of the loss with respect to each parameter by applying the chain rule of calculus through each layer in reverse â€”from output back to inputâ€”so that it can determine how much each weight contributed to the error; these gradients indicate the direction in which each weight should be changed to reduce the loss, and in the update step, each parameter is adjusted slightly in the opposite direction of its gradient (hence \"descent\") using a learning rate, which controls the size of these updates to ensure convergence without overshooting; this cycle of forward pass, loss computation, backpropagation, and weight update is repeated across many epochs over the training dataset, allowing the neural network to iteratively learn patterns in the data, gradually reducing the loss and improving its ability to generalize to unseen inputs, provided the model architecture is appropriate and overfitting or underfitting is mitigated through techniques like regularization, dropout, or validation.',\n",
       " '2. What is Regulariztion?',\n",
       " \"Explain briefly the types of regularization Regularization in neural networks is a set of techniques used to prevent overfitting, which occurs when the model learns not only the underlying patterns in the training data but also the noise or random fluctuations, leading to poor generalization on unseen data; the core idea behind regularization is to add constraints or penalties to the model's complexity during training, typically by modifying the loss function so that, in addition to minimizing prediction error, the model is also discouraged from having excessively large weights or overly complex behavior; one common form is L2 regularization (also called weight decay), where a term proportional to the sum of the squares of all weights is added to the loss function, effectively penalizing large weights and encouraging the network to distribute learning more evenly across all features, resulting in smoother and more general decision boundaries.\",\n",
       " 'Another variant is L1 regularization, which adds the absolute values of weights to the loss, promoting sparsity by pushing some weights toward zero and effectively performing feature selection; beyond these, dropout is a powerful regularization technique where, during training, a random subset of neurons is \"dropped\"',\n",
       " ', temporarily ignored) in each forward and backward pass, which forces the network to not rely too heavily on specific paths and promotes redundancy and robustness in learning; early stopping is another method where training is halted once performance on a validation set stops improving, thus avoiding the point at which the model starts to overfit; collectively, these regularization strategies are crucial for building models that generalize well, especially in deep learning where the number of parameters is large and the risk of overfitting is high if the model becomes too specialized on the training set.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def split_into_chunks(text, max_chars=500):\n",
    "    sentences = re.findall(r'[^.!?]+[.!?]?(?=\\s+|$)', text)\n",
    "    \n",
    "    chunks = []\n",
    "    current = \"\"\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        if len(current) + len(sentence) <= max_chars:\n",
    "            current += \" \" + sentence.strip()\n",
    "        else:\n",
    "            if current:\n",
    "                chunks.append(current.strip())\n",
    "            current = sentence.strip()\n",
    "    \n",
    "    if current:\n",
    "        chunks.append(current.strip())\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "paragraphs = []\n",
    "for page in text_chunks:\n",
    "    paragraphs.extend(split_into_chunks(page))\n",
    "paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a03d77-7558-47fb-8794-ab2251901f32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f301b1a81b344f72a79bdbc68b78fcd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index built.\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def embed(text):\n",
    "    return embedding_model.encode([text])[0] \n",
    "\n",
    "embeddings = model.encode(paragraphs, show_progress_bar=True)\n",
    "dimension = embeddings[0].shape[0]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(np.array(embeddings))\n",
    "\n",
    "print(\"FAISS index built.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d9c88b-2145-4c5e-acd3-ba917be47398",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf21a47-b341-421b-8d2b-241c20196923",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"deepset/roberta-base-squad2\")\n",
    "\n",
    "def answer_question(question, top_k=10):\n",
    "    question_embedding = model.encode([question])\n",
    "    distances, indices = index.search(np.array(question_embedding), top_k)\n",
    "    \n",
    "    top_chunks = [paragraphs[i] for i in indices[0]]\n",
    "    \n",
    "    for i, chunk in enumerate(top_chunks):\n",
    "        print(f\"\\nContext {i+1}:\\n{chunk[:300]}...\")\n",
    "    \n",
    "    answers = []\n",
    "    for context in top_chunks:\n",
    "        answer = qa_pipeline(question=question, context=context)\n",
    "        answers.append((answer[\"answer\"], answer[\"score\"]))\n",
    "    \n",
    "    answers.sort(key=lambda x: -x[1])\n",
    "    return answers[0][0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2b231f5-dddf-4aed-8451-d34694795fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Context 1:\n",
      "The loss function (such as mean squared error for regression or cross-entropy for classification) is then computed to measure how far off the predictions are from the ground truth; next comes the backward pass, known as backpropagation, where the network calculates the gradient of the loss with resp...\n",
      "\n",
      "Context 2:\n",
      "2. What is Regulariztion?...\n",
      "\n",
      "Context 3:\n",
      "Gradient descent in a neural network is a fundamental optimization technique used to train the network by iteratively adjusting its parameters (weights and biases) to minimize a loss function, which quantifies the error between the network's predictions and the actual target values; the process begi...\n",
      "\n",
      "Context 4:\n",
      ", temporarily ignored) in each forward and backward pass, which forces the network to not rely too heavily on specific paths and promotes redundancy and robustness in learning; early stopping is another method where training is halted once performance on a validation set stops improving, thus avoidi...\n",
      "\n",
      "Context 5:\n",
      "1. What is Gradient descent?...\n",
      "\n",
      "Context 6:\n",
      "Explain briefly the types of regularization Regularization in neural networks is a set of techniques used to prevent overfitting, which occurs when the model learns not only the underlying patterns in the training data but also the noise or random fluctuations, leading to poor generalization on unse...\n",
      "\n",
      "Context 7:\n",
      "Another variant is L1 regularization, which adds the absolute values of weights to the loss, promoting sparsity by pushing some weights toward zero and effectively performing feature selection; beyond these, dropout is a powerful regularization technique where, during training, a random subset of ne...\n",
      "\n",
      "Context 8:\n",
      ", temporarily ignored) in each forward and backward pass, which forces the network to not rely too heavily on specific paths and promotes redundancy and robustness in learning; early stopping is another method where training is halted once performance on a validation set stops improving, thus avoidi...\n",
      "\n",
      "Context 9:\n",
      ", temporarily ignored) in each forward and backward pass, which forces the network to not rely too heavily on specific paths and promotes redundancy and robustness in learning; early stopping is another method where training is halted once performance on a validation set stops improving, thus avoidi...\n",
      "\n",
      "Context 10:\n",
      ", temporarily ignored) in each forward and backward pass, which forces the network to not rely too heavily on specific paths and promotes redundancy and robustness in learning; early stopping is another method where training is halted once performance on a validation set stops improving, thus avoidi...\n",
      "\n",
      "ðŸ§  Answer: backward pass\n"
     ]
    }
   ],
   "source": [
    "question = \"What is back propgation?\"\n",
    "answer = answer_question(question)\n",
    "print(f\"\\nðŸ§  Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca40f8f7-33eb-4b74-8858-9f791d2c5169",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“ Summary:\n",
      "Gradient descent in a neural network is a fundamental optimization technique used to train the network. It is used to minimize a loss function, which quantifies the error between the network's predictions and the actual target values. Regularization in neural networks is a set of techniques used to prevent overfitting.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "long_text = \" \".join(paragraphs[:5])\n",
    "summary = summarizer(long_text, max_length=250, min_length=30, do_sample=False)[0][\"summary_text\"]\n",
    "print(f\"\\nðŸ“ Summary:\\n{summary}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2706b8f8-b6fd-4eca-8e2d-34376159849d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61552320-8321-44f9-b261-7eb56b15e5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain import HuggingFaceHub \n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"HUGGINGFACEHUB_API_TOKEN\" \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = \"HInfinity_Final.pdf\"\n",
    "    loader = PyPDFLoader(file_path=pdf_path)\n",
    "    document = loader.load()\n",
    "\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=30)\n",
    "    doc = text_splitter.split_documents(documents=document)\n",
    "\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")  \n",
    "    vectorstore = FAISS.from_documents(documents=doc, embedding=embeddings)\n",
    "    vectorstore.save_local(\"QML-Learnings\")\n",
    "    \n",
    "\n",
    "    generator = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\")\n",
    "    \n",
    "    llm = HuggingFacePipeline(pipeline=generator)\n",
    "\n",
    "\n",
    "    new_vectorstore = FAISS.load_local(\"QML-Learnings\", embeddings,allow_dangerous_deserialization=True)\n",
    "    qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=new_vectorstore.as_retriever())\n",
    "    print(qa.input_keys)\n",
    "\n",
    "    user_query = \"\"\n",
    "\n",
    "    while user_query.lower() != \"thank you\":\n",
    "        user_query = input(\"Ask: \")\n",
    "        result = qa.run(user_query)\n",
    "\n",
    "        print(\"Answer:\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4331f2-8ff0-431f-ab3c-6833bf8f489c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gc/6kg_20fs4b7c4rvdzjj6vgl00000gn/T/ipykernel_76197/4245783612.py:36: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
      "Device set to use mps:0\n",
      "/var/folders/gc/6kg_20fs4b7c4rvdzjj6vgl00000gn/T/ipykernel_76197/4245783612.py:43: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=generator)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ask:  What is the F1 score of proposed model?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gc/6kg_20fs4b7c4rvdzjj6vgl00000gn/T/ipykernel_76197/4245783612.py:52: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = qa.run(user_query)\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4847 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from transformers import pipeline\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\" \n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.retrieval_qa.base import RetrievalQA\n",
    "\n",
    "prompt_template = \"\"\"You are a helpful AI assistant. Use the context below to answer the question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(input_variables=[\"context\", \"question\"], template=prompt_template)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = \"HInfinity_Final.pdf\"\n",
    "    loader = PyPDFLoader(file_path=pdf_path)\n",
    "    document = loader.load()\n",
    "\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=30)\n",
    "    doc = text_splitter.split_documents(documents=document)\n",
    "\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\") \n",
    "    vectorstore = FAISS.from_documents(documents=doc, embedding=embeddings)\n",
    "    vectorstore.save_local(\"QML-Learnings\")\n",
    "    \n",
    "    generator = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\")\n",
    "    # generator = pipeline(\"text2text-generation\", model=\"meta-llama/Llama-2-7b-chat-hf\") #[\"meta-llama/Llama-2-7b-chat-hf\", \"google/flan-t5-base\"]\n",
    "    \n",
    "    llm = HuggingFacePipeline(pipeline=generator)\n",
    "\n",
    "    new_vectorstore = FAISS.load_local(\"QML-Learnings\", embeddings,allow_dangerous_deserialization=True)\n",
    "    qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=new_vectorstore.as_retriever())\n",
    "\n",
    "    user_query = \"\"\n",
    "\n",
    "    while user_query.lower() != \"thank you\":\n",
    "        user_query = input(\"\\nAsk: \")\n",
    "        result = qa.run(user_query)\n",
    "        print(\"\\nAnswer:\", result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5fa69117-4cf1-48a4-b4e6-9767eb6acbb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: This is a dummy response.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd1b567-99d0-48d4-8d8f-b79c25157642",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
